## 1 可视化

### 1.1 散点图+预测函数or决策边界

```
用来初步识别X和Y的关系，以及假设函数和训练集的匹配度,适合X低维的情况。
```



![线性回归散点图](pictures\p21.png)



![逻辑回归散点图](pictures\p23.png)

![逻辑回归散点图](pictures\p24.png)

### 1.2 J收敛曲线

```
用来评估梯度下降中J的收敛情况，对学习速率进行调整。
```

![梯度下降中带价值J随迭代次数的变化](pictures\p22.png)

### 1.3 学习曲线

```
标记出训练集和交叉验证集的误差，随着训练样本数量或者多项式次数或正则化参数labmda的变化曲线。
帮助我们选择合适的学习参数
```

![梯度下降中带价值J随迭代次数的变化](pictures\p25.png)

![梯度下降中带价值J随lambda的变化](pictures\p26.png)

## 2 评估调试

训练集（60%）：训练数据。

交叉验证集（20%）：阶段性评估算法，调整学习参数。

测试集（20%）：最终评估算法。

### 2.1 偏差(bias)与方差(variance)

```
高偏差低方差意味着目前的假设处于欠拟合的状态，对训练集的数据都不能很好的适应。低偏差高方差意味着目前的假设处于过拟合的状态，对训练集数据过分拟合，难以适应训练集以外的数据。
```

![bias/variance](pictures\p28.png)

#### 2.1.1 与多项式次数d的关系



![bias/variance](pictures\p27.png)

#### 2.1.2 与正则化参数lambda的关系

![bias/variance](pictures\p29.png)

#### 2.1.3 与训练集数量m的关系

```
如果学习算法处于高偏差的状态，增加训练数据无效。
如果学习算法处于高方差的状态，增加训练数据可能会有效。
```

![bias/variance](pictures\p30.png)

![bias/variance](pictures\p31.png)





#### 2.1.4 下一步优化

- 增加更多的训练数据->解决高方差
- 用更少的特征->解决高方差
- 增加额外的特征->解决高偏差
- 增加多项式项->解决高偏差
- 增大lambda->解决高方差
- 减小lambda->解决高偏差

### 2.2 神经网络的优化

- 结构简单、参数少->高偏差=计算量小
- 结构复杂、参数多->高方差=计算量大->增加正则化项

### 2.3 机器学习算法的构建流程（误差分析）

1. 从一个简单的算法开始，快速的实现，在交叉验证集上测试。
2. 画出学习曲线，来决定下一步的优化方式。
3. 手动分析出交叉验证集中的错误，找到错误的类型，提供下一步的优化思路。

### 2.4 偏斜数据

```
训练集数据本身是偏斜状态的时候，光用误差来评估算法的好坏就不太足够。还需要用F score来衡量。
precision = true positive/(true positive + false positive)
recall = true positive/(true positive + false negtive)
F1 score = 2PR/(P+R)
```

![bias/variance](pictures\p32.png)

### 2.5 上限分析(ceiling analysis)

```
问题：流水线中的哪个部分值得你花更多的资源提升。
流程：给每个环节输入正确的数据，并统计系统最终的性能评价，来发现哪个环节的优化效率使最高的。
```

![bias/variance](pictures\p37.png)

