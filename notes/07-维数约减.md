## 1 模型描述

```
作用：数据降维，比如把训练集数据从3D降到2D
```

![线性回归散点图](pictures\p34.png)

## 2 主成分分析

```
一种数据降维方法，把1000D的数据，保留其中离散度最大的面(100D)，将数据映射到这个面(100D)中。
具体来说，是找到一个方向(向量)，使所有数据到这个方向的距离和最小。
```

![线性回归散点图](pictures\p35.png)

### 2.1 数据压缩

$$
首先进行均值归一化和特征缩放(可选)									\\
协方差矩阵：\Sigma = \frac{1}{m}\sum^n_{i=1}(x^{(i)})(x^{(i)})^T	\\
奇异值分解:[U,S,D] = svd(\Sigma)									\\
取出前K列的数据:U_{reduce} = U(:，1:k)								\\
计算z = U_{reduce}' * x
$$

### 2.2 数据还原

$$
X_{approx} = U_{reduce} * z
$$

![线性回归散点图](pictures\p36.png)

## 3 K值选择

$$
主成分数量:k	\\
最小化平均平方映射误差:\frac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2	\\
数据总变差:\frac{1}{m}\sum^m_{i=1}||x^{(i)}||^2	\\
选择能满足 \frac{最小化平均平方映射误差}{数据总变差} <=0.01 (阈值) 最小的k 	\\
表示百分之99的差异性得到了保留	\\
-----------------------------------	\\
等价于：\\
[U,S,D] = svd(\Sigma)	\\
smallest \space k \space for: \space 1-\frac{\sum^k_{i=1}S_{ii}}{\sum^m_{i=1}S_{ii}} <= 0.01
$$



## 4 使用建议

```
好的应用：
1、数据降维，比如图像像素，增加监督学习的速度。
2、减少内存、硬盘使用

不好的应用：
1、不适合解决过拟合的问题，容易丢失有价值的信息。
2、使用pca前最好用元数据，如果不合适再用pca
```

## 5 代码

```octave
特征归一化：
function [X_norm, mu, sigma] = featureNormalize(X)

mu = mean(X);
X_norm = bsxfun(@minus, X, mu);

sigma = std(X_norm);
X_norm = bsxfun(@rdivide, X_norm, sigma);

end
```

```octave
pca主流程：
function [U, S] = pca(X)

[m, n] = size(X);

Sigma = X' * X /m;
[U, S, V] = svd(Sigma);

end
```

```octave
数据压缩:
function Z = projectData(X, U, K)

Z = X * U(:, 1:K);

end
```

```octave
数据恢复:
function X_rec = recoverData(Z, U, K)

X_rec = zeros(size(Z, 1), size(U, 1));

X_rec = Z * U(:, 1:K)';

end
```